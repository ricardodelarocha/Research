# Free Jailbreaks in LLMs

This work contains sensitive material, but its purpose is purely didactic. All experiments were performed responsibly in controlled environments, with no intent of malicious use. Readers are strongly discouraged from replicating these actions outside academic or ethical research contexts.

## About this repository
This repository gathers the appendices of **_Exploring Jailbreaks in Large Language Models: An Experimental Analysis of Safety Barriers_** paper.

## Contents

* **Prompt Injection Attacks (Jailbreaks):**
    * Files detailing various **prompt injection** and **jailbreaking techniques** applied across multiple LLMs (e.g., Gemini, ChatGPT, GPT-4o mini, Kimi, Mistral Small 3).
    * Demonstrations of different attack vectors, including **indirect prompt injection**, **malware delivery simulations**, **pirated content generation**, and **DDoS attack simulations**.
* **Didactic and Supplemental Materials:**
    * A lesson plan focused on LLM attacks (in Portuguese).
    * A collection of shared URLs relevant to the research.
* **Experimental Output and Documentation:**
    * Visual documentation of experimental inputs and outputs, including edited and original images, and a full prompt screen capture for analysis.

## Backup
- [Aqui](under construction)

## Notes
These materials are supplementary to the main paper and are provided for transparency and replication purposes.
